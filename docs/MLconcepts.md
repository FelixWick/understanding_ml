# Understanding Machine Learning

It just takes a few concepts. (Admittedly, the following brief descriptions require some familiarity with the topic.)

## Instructions out of Data

On a high level, a machine learning training can be interpreted as generating an explicit algorithm (to be used for inference) from a more abstract (and simpler) learning algorithm and data, by exploiting statistical dependencies in the training data set. Compared to traditional, handcrafted algorithms (often relying on detailed domain knowledge), this usually leads to a much better generalizability to new situations.

## Empirical Risk Minimization

Most ML algorithms (not only supervised, but also unsupervised and reinforcement learning) can be described by the general recipe of combining models, costs, optimization, and regularization methods, with the goal of generalization. For this, the minimization of the training error (on empirical data) serves as proxy for the minimization of the unknown test error (population risk).

## Manifold Hypothesis

Generalization in high dimensions (models with many features) usually amounts to extrapolation (curse of dimensionality). But fortunately, reality is friendly: Most high-dimensional data sets arguably reside on lower-dimensional manifolds, what enables the effectiveness of empirical risk minimization (interpolation).

## Inductive Bias

To generalize well for a given task, one needs to find a learning algorithm with an appropriate set of assumptions (inductive biases) to predict outputs of inputs not encountered during training (what can be interpreted as data in disguise). Inductive bias can come in many different forms: model design (e.g., linear response in linear regression, nearest neighbors in kNN), regularization (e.g., convolutions in CNN), optimization algorithms, etc.

## Linear Building Blocks

Most powerful, non-linear ML algorithms (such as neural networks or decision trees) are compound, with rather simple (often linear) building blocks. This can be seen as reductionism of complex systems with (implicit and automatic) interaction modeling.

## i.i.d. assumption

It is assumed that the data samples, i.e., each individual observation, are generated by repeatedly and independently drawing from a single (identically distributed) set of random variables (for supervised learning: target Y and features **X**). While the identity assumption is the basis of empirical risk minimization, requiring consistency between training and test data sets (what is not always the case, leading to suboptimal results), the independence assumption excludes intersample correlations from the model (Don't be confused by sequence samples, such as time series or text passages, it's the same thing with several time steps or words in one sample.), oversimplifying certain scenarios like Markov decision processes in reinforcement learning or causal interventions.

## It's all about Supervised Learning

As sample efficiency is a notorious problem in ML, it needs big data sets to train high-capacity models, which can be challenging for the supervised-learning setup in terms of labeling (defining the target variable). However, for many tasks there exist vast amounts of unlabeled data (such as text in the internet), which can be used to generate the target variable itself (e.g., next-word prediction in language models). This corresponds to framing unsupervised learning in the supervised-learning setup.

The reinforcement-learning setup (a search), that is finding a good action policy in sequential decision making, is usually more difficult (e.g., non-differentiable as a whole) than the supervised-learning one (an optimization). But technically, it can also be cast as supervised learning, by expressing the rewards of the environment in more intricate loss functions (examples: Q-values as target, weighting of policy gradients by rewards).

## Representation Learning

On top of merely mapping inputs (in the form of potentially engineered features) to outputs, deep learning methods (neural networks with many layers) aim to learn abstract, distributed representations directly from raw input data (aka feature learning), a prominent example being word embeddings (or in fact, the full encoder part) in language models. (But the same approach can also be applied to graph data structures in general.) Thanks to this independence from specific feature extractions, modern, high-capacity representation-learning algorithms (such as large language models) are capable of multi-task learning, i.e., performing beyond the narrow domain of individual tasks (potentially improved via fine-tuning or in-context learning), and can even operate across different modalities like text and images (multi-modal learning).

## Discriminative vs Generative Models

Discriminative models predict the conditional probability (or probability distribution for regression tasks) P(Y\|**X**). (In fact, the output is often just the most probable label for classification or the real value representing for example the mean of the assumed probability distribution for regression.) Generative models predict the joint probability P(Y,**X**), what allows the generation of new data samples, be it text, code, images, video, proteins, materials, etc. In this sense, large (auto-regressive) language models (usually decoder-only transformers) and systems generating synthetic images (e.g., by learning variational distributions) are examples of generative models. Since it aims to model the full data distribution, rather than merely finding patterns in the inputs to distinguish (discriminate) outputs, generative modeling is more difficult. This explains why discriminative methods are usually superior for discriminative tasks, although generative methods can be used for it, too.
